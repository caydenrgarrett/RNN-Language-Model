# -*- coding: utf-8 -*-
"""RNN Language Model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a_bBGCUzr0pTmU5zUTw3cvW0V5ZpcPYw

### Import & Load Dataset
"""

!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O tiny_shakespeare.txt

with open("tiny_shakespeare.txt", "r", encoding="utf-8") as f:
    text = f.read()

print(text[:500])

"""### Build the vocabulary"""

chars = sorted(list(set(text)))
vocab_size = len(chars)
stoi = {ch:i for i,ch in enumerate(chars)}
itos = {i:ch for ch,i in stoi.items()}

def encode(s):
  return [stoi[c] for c in s]

def decode(l):
  return "".join([itos[i] for i in l])

"""### Create Training Batches"""

import torch

data = torch.tensor(encode(text), dtype=torch.long)
block_size = 64
batch_size = 32

def get_batch():
  idx = torch.randint(len(data) - block_size, (batch_size,))
  x = torch.stack([data[i:i+block_size] for i in idx])
  y = torch.stack([data[i+1:i+block_size+1] for i in idx])
  return x, y

"""### Defining RNN Model"""

import torch.nn as nn

class RNNLanguageModel(nn.Module):
  def __init__(self, vocab_size, hidden_size=128):
    super().__init__()
    self.embed = nn.Embedding(vocab_size, hidden_size)
    self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)
    self.fc = nn.Linear(hidden_size, vocab_size)

  def forward(self, x, h=None):
    x = self.embed(x)
    out, h = self.rnn(x, h)
    logits = self.fc(out)
    return logits, h

"""### Training the RNN"""

model = RNNLanguageModel(vocab_size)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
criterion = nn.CrossEntropyLoss()

for step in range(2000):
  x, y = get_batch()

  logits, _ = model(x)
  loss = criterion(logits.view(-1, vocab_size), y.view(-1))

  optimizer.zero_grad()
  loss.backward()
  optimizer.step()

  if step % 200 == 0:
    print(f"Step {step}, loss {loss.item():.4f}")

"""### Generate Text"""

model.eval()
context = torch.tensor([encode("KING: ")], dtype=torch.long) # Corrected typo and removed .cuda()
h = None

for _ in range(300):
  logits, h = model(context, h)
  probs = torch.softmax(logits[:, -1, :], dim=-1) # Corrected softmax calculation
  idx = torch.multinomial(probs, num_samples=1)
  context = torch.cat([context, idx], dim=1) # Corrected typo

print(decode(context[0].tolist()))