{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Import & Load Dataset"
      ],
      "metadata": {
        "id": "Wa-Du47vBT6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O tiny_shakespeare.txt\n",
        "\n",
        "with open(\"tiny_shakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(text[:500])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdNlwiVOO_o6",
        "outputId": "a5c22d3e-ba84-43e1-ff19-a25cf3525bd1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-08 16:17:42--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘tiny_shakespeare.txt’\n",
            "\n",
            "\rtiny_shakespeare.tx   0%[                    ]       0  --.-KB/s               \rtiny_shakespeare.tx 100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-11-08 16:17:42 (19.8 MB/s) - ‘tiny_shakespeare.txt’ saved [1115394/1115394]\n",
            "\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the vocabulary"
      ],
      "metadata": {
        "id": "R2Kw8MR4CsUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for ch,i in stoi.items()}\n",
        "\n",
        "def encode(s):\n",
        "  return [stoi[c] for c in s]\n",
        "\n",
        "def decode(l):\n",
        "  return \"\".join([itos[i] for i in l])"
      ],
      "metadata": {
        "id": "JrDFDrVECujo"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Training Batches"
      ],
      "metadata": {
        "id": "eBxPjJ2gFBs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "block_size = 64\n",
        "batch_size = 32\n",
        "\n",
        "def get_batch():\n",
        "  idx = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in idx])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in idx])\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "mEhuphNEFGcc"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining RNN Model"
      ],
      "metadata": {
        "id": "yFdO8tfPGcDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNNLanguageModel(nn.Module):\n",
        "  def __init__(self, vocab_size, hidden_size=128):\n",
        "    super().__init__()\n",
        "    self.embed = nn.Embedding(vocab_size, hidden_size)\n",
        "    self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "  def forward(self, x, h=None):\n",
        "    x = self.embed(x)\n",
        "    out, h = self.rnn(x, h)\n",
        "    logits = self.fc(out)\n",
        "    return logits, h"
      ],
      "metadata": {
        "id": "TZVO2MszGdqv"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the RNN"
      ],
      "metadata": {
        "id": "6SBY5t6GIeTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNNLanguageModel(vocab_size)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for step in range(2000):\n",
        "  x, y = get_batch()\n",
        "\n",
        "  logits, _ = model(x)\n",
        "  loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if step % 200 == 0:\n",
        "    print(f\"Step {step}, loss {loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6lLDYiUIgdB",
        "outputId": "81d39534-0b46-432b-b895-7afbe3197f6c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0, loss 4.2498\n",
            "Step 200, loss 2.1256\n",
            "Step 400, loss 1.9521\n",
            "Step 600, loss 1.8858\n",
            "Step 800, loss 1.8207\n",
            "Step 1000, loss 1.8082\n",
            "Step 1200, loss 1.7318\n",
            "Step 1400, loss 1.7691\n",
            "Step 1600, loss 1.6490\n",
            "Step 1800, loss 1.7549\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Text"
      ],
      "metadata": {
        "id": "GpaNUemZLJz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "context = torch.tensor([encode(\"KING: \")], dtype=torch.long) # Corrected typo and removed .cuda()\n",
        "h = None\n",
        "\n",
        "for _ in range(300):\n",
        "  logits, h = model(context, h)\n",
        "  probs = torch.softmax(logits[:, -1, :], dim=-1) # Corrected softmax calculation\n",
        "  idx = torch.multinomial(probs, num_samples=1)\n",
        "  context = torch.cat([context, idx], dim=1) # Corrected typo\n",
        "\n",
        "print(decode(context[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eR8vMI8KLLj1",
        "outputId": "5c6a22bf-fdd2-4ca3-ab32-9cb1a1194da8"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KING: let? so how and father? ba hend Birtom's will enco for the juys lasting in fire one fair nod thy hearn my fear dames letters thy orle.\n",
            "The simes doth'd adgitert. The cheek so pleath?\n",
            "\n",
            "TRANION:\n",
            "My thy France of enjuch good\n",
            "Ang fans!\n",
            "\n",
            "ANGELO:\n",
            "Poly sweet cyodalo in peven a have underaclest with gralken\n"
          ]
        }
      ]
    }
  ]
}